<h3>ABSTRACT</h3>
<p>People often ask others for restaurant recommendations as a way to discover new dining experiences. This makes restaurant recommendation an exciting scenario for recommender systems and has led to substantial research in this area.</p>
<p>However, most such systems behave very differently from a human when asked for a recommendation. The goal of this paper is to begin to reduce this gap.</p>
<p>In particular, humans can quickly establish preferences when asked to make a recommendation for someone they do not know. We address this cold-start recommendation problem in an online learning setting. We develop a preference elicitation framework to identify which questions to ask a new user to quickly learn their preferences. Taking advantage of latent structure in the recommendation space using a probabilistic latent factor model, our experiments with both synthetic and real world data compare different types of feedback and question selection strategies. We find that our framework can make very effective use of online user feedback, improving personalized recommendations over a static model by 25% after asking only 2 questions. Our results demonstrate dramatic benefits of starting from offline embeddings, and highlight the benefit of bandit-based explore-exploit strategies in this setting.</p>
<h3>Keywords</h3>
<p>online learning; recommender systems; cold-start</p>

<h3>1. INTRODUCTION</h3>
<p>Recommendation is an everyday process that frequently touches people＊s lives. Hence, it has seen tremendous research interest (such as [9, 14]). Most work in recommendation falls into two broad classes: Collaborative Filtering starts with a set of user/item affinity scores and assumes that two users who agree about one item are more likely to agree about another item; Content-Based Filtering models users by the characteristics of the items they like or dislike.</p>
<p>We note that neither model represents how real people make recommendations, particularly in a cold-start setting where the person making a recommendation does not know a lot about the person asking for one.</p>
<p>Consider what would happen if a conference attendee in your home town, whom you have never met before, asked for a recommendation on where to eat dinner today. Most likely, you would start with one or two clarifying questions, perhaps whether the person likes seafood, or whether they have a car. These questions would depend on the context; for instance if there are great restaurants around the corner, then whether they have a car would be irrelevant. </p>
<p>We argue that such an interaction can be represented using online learning, where two types of learning are occurring. First, the person making the recommendation is learning about the preferences of the person asking. However, the attributes learned will be contextual, based on the likely follow-on answers (such as the car question earlier). Second, the person making the recommendation is learning about which questions allow them to quickly reach a good recommendation in the current context. In this paper, we present a recommendation system that exhibits these two types of learning. Further, the learning is online: It immediately impacts future recommendations for this user, rather than requiring a batch reprocessing of information learned. </p>
<p>We present a bandit-based approach for online recommendation, applied to restaurant recommendation so as to ground it in a specific application. Our approach builds on top of prior work, such as [33], but learns to adapt the recommendation space (user-item embedding) to its users throughout their interactions. We use generalized Thompson Sampling to systematically sample questions to ask the user and to incorporate observed feedback. We further propose and compare a range of alternative question selection strategies to identify characteristics of approaches that most effectively learn users＊ preferences. We use a matrix factorization approach [21, 29] to learn and adapt the embedding. </p>
<p>Our main contributions are four-fold. (1) We propose a novel view of human-like recommenders that converse with new users to learn their preferences. (2) We successfully demonstrate a fully online learning approach for recommendation 每 both using absolute and relative feedback. (3) We propose a systematic approach to incorporating offine data to initialize online learning recommenders, and demonstrate performance improvements, even using weakly labeled offline data. (4) We propose a set of item selection strategies for deciding what question to ask to a cold-start user to most quickly infer preferences, and demonstrate benefits of bandit-based strategies. Our extensive experiments on both synthetic and real data evaluate each step of our approach.</p>
<p>Importantly, we note that this work is applicable to a wide variety of recommendation scenarios. Here, we focus on one such application, restaurant recommendation, where (1) we study search logs to understand the space of real user needs; (2) we use the insights to collect preference data in a user study; (3) we use online behavioral data to initialize the recommendation system; (4) we use both synthetic and user study data to evaluate our approaches.</p>
<p>We now present related work (Section 2), followed by an analysis of real-live restaurant search (3). We describe our model (4) and empirical setup (5), and validate the model on synthetic data (6). Finally, we evaluate on real data to further empirically analyze our learning framework (7).</p>

<h3>2. RELATED WORK</h3>
<p>This paper builds on work in multiple active research areas.We review recent work in the most closely related areas.</p>
<p>Offine Recommenders. The wide interest in personalized recommendations has sparked substantial research in this area [14]. The most common approaches are contentbased approaches [24] and collaborative filtering (CF) [9, 21]. Collaborative filtering, which powers most modern recommenders, uses an a-priori available set of user-item ratings to learn the interdependencies among users and items. It predicts a user＊s rating on an item either via the neighboring items＊ ratings (neighbor-based [9, 28]) or by inferring latent factors in a low-dimensional embedding (latent factorbased [21, 29]). The majority of the work in recommendation has focused on offine recommenders, i.e., building an offine model based on past user-item interactions, periodically retrained to incorporate new observations.</p>
<p>Online Recommenders. Recently, it has been recognized that offine recommender approaches (i) suffer from the cost of retraining the model, (ii) are built to optimize offine performance which does not necessarily match online user behavior and (iii) fail to capture preference drifts.These realizations have initiated developments towards building continuously learning (online) recommenders.</p>
<p>A recent line of work poses the problem in a contextual bandit formulation [16, 31, 32], where items are seen as arms, users as contexts, and the goal is to explore the arm space in order to exploit the best performing arm for a given context. Most work in this area relies on the availability of user/item features and assumes that the reward of an arm for a given context is a linear/logistic function of the concatenated feature of arm-context [16, 31]. [32] uses Gaussian processes to allow feedback sharing among similar contexts and arms.</p>
<p>Using the CF point of view, [4] introduced an epslon-greedy online user-user neighbor-based CF method. The first latentfactor online recommender was introduced in [33], which uses bandit strategies on top of Probabilistic Matrix Factorization (PMF) [21]. This is the most closely related work to ours. However, while [33] fixes the item latent factors to those learned offine, formulating the problem as a linear bandit, our method fully online learns all user and item parameters, including the biases; [33] can be seen as special case of our framework. In [13], the authors extend Thompson Sampling for PMF with a Kalman filter to track changing preferences over time. This work is orthogonal to ours. </p>
<p>Preference Elicitation. The problem of eliciting user feedback has long been of interest for a variety of tasks (e.g [8]). To elicit the preferences of an existing or new user (cold-start), a range of methods have been proposed, varying from interview-based strategies (e.g [30]), to asking users to rate some items, to active learning [26], entropy minimization [27], picture-based [22], and explore-exploit strategies on top of a latent factor model [33]. Our work is the first to elicit users＊ preferences by utilizing either absolute or relative feedback in a fully online setting. </p>
<p>Interactive Recommenders. There have been many works (critique-based [7], constraint-based [10], dialog, utilitybased recommenders [19]) emphasizing the importance of interactivity in recommenders so that the user has a more active role over the recommendations. However, these works rely on prior modeling of the items＊ features, preventing the flexibility in adaptation to a different domain; thus a comparison with them is out of the scope of this work.</p>
<p>In contrast, our work, in the same spirit as a recent line of work [18, 11] initiated by [33], learns online the latent factors from PMF and uses these to do interactive preference elicitation. These methods, although close in principle, have significant differences from our work.1 Briefly, in [18], the authors focus on set-based feedback and propose a progressive building of the user＊s latent factor in each question. In contrast, our work focuses on absolute and relative feedback on (pairs of) items, and uses a preference elicitation phase fully integrated with the online updates of the PMF model. The method of [11] focuses on choice-based feedback and updates online only the user＊s latent factor. Neither of [11, 18] balance the exploration-exploitation tradeoff. </p>

<h3>3. UNDERSTANDING REAL USERS</h3>
<p>A recommendation system that aims to satisfy real people should reflect how real people look for recommendations. Therefore, we start by analyzing restaurant-related search behavior in a commercial Web search engine. Our goals are two-fold. First, to gain insight into our target domain, namely understand the criteria people use to choose restaurants. Second, to identify questions we must ask users to construct ground truth data for evaluating our system.</p>
<p>Given a large sample of all queries issued between 07/2014 and 07/2015, we filtered for those that contain restaurant or dining, and terms such as for, near(by), next (to), close (to), with and in. Let Q be the most frequent 10,000 such queries.</p>

<h4>3.1 Query Annotation with Entities</h4>
<p>We tagged the top several hundred queries from Q as containing a location, name, cuisine, food type, or terms such as best and menu. The dictionary of tags was not pre-specified to avoid biasing our annotations due to prior beliefs about categories people should be looking for. Rather, our methodology used content analysis [25] to develop a coding that captures the dominant behaviors involved. </p>
<p>We found that 39% of queries specify a location, 19% a restaurant name, 9% cuisine constraints, 7% have the term best or other superlative. More rarely, queries specified food ethnicity (2%), an adjective describing the restaurant (2%), dish name (2%), decoration, etc. The most common term cooccurences are location and name in the same query (29%), cuisine and location (10%), and best and location (8%). </p>

<h4>3.2 Understanding Common Phrases</h4>
<p>The above statistics show that location is an important factor in restaurant search. Hence we zoom in to discover the specific ways in which people constrain locations. Similarly, the context under which users search for a restaurant is a second common constraint. We analyzed the specific terms appearing after a variety of prepositions, and constructed a dictionary of the most frequent contextual constraints. </p>
<p>A sample of these is shown in Table 1. For example, the top places people search for restaurants in are nyc, chicago, las vegas. Using the preposition ＆near＊ to indicate location, the majority of terms shows users want a restaurant near me. Queries can also be very specific, e.g. searching for a restaurant near points of interest (times square), airports (miami airport, lax ), postal codes or entire addresses. The contexts under which users search for restaurants vary from an occasion (wedding reception), to dietary restrictions, to type of meal or a group of people. People also search for restaurants with live music, piano, a view etc. </p>

<h4>3.3 Restaurant Feature Dictionaries</h4>
<p>As user queries can be very specific (e.g., combining constraints, ＆adjective & dish & great & location＊), we now study the terms that people use to describe restaurants. We annotated the top 1,013 phrases appearing before the word ＆restaurant＊ or ＆dining＊ in 5,000 queries from Q with a number of traits. The traits identified are related to food (cuisine, meal, dietary restrictions, quality e.g. organic, healthy, buffet, menu), rating (e.g michelin star), atmosphere (view, fancy or romantic), time/location (opening hours, decade theme, time of the day) etc. </p>
<p>For every trait, we collected the most common terms. Table 2 shows a few common examples, giving us a glimpse into the most searched cuisines, food types, and adjectives. Though not shown, top amenities searched are garden, jazz, patio, ocean and top restaurant types are bar, fast food, cafe.</p>

<h4>3.4 Outlook</h4>
<p>Besides motivating our application scenario, this search log analysis forms the basis of the user study in Section 7. Given this understanding of what to ask people who look for a restaurant, we also focus on how to ask it. In this work, among the various combinations of the feedback type (explicit/implicit, absolute/relative/list) on the available content (explicit features/items/latent features), we elicit users＊ preferences using explicit feedback from absolute and relative questions about explicit items (e.g. restaurants). </p>

<h3>4. MODEL</h3>
<p>We next present our approach for determining ＆what to ask＊ and ＆how to ask＊ as the key pieces of a conversational recommender, starting with a high level picture of the entire algorithm (Section 4.1). Then, we describe the pieces in detail as we require (i) a model exploiting implicit structure among items and users to eciently propagate feedback (Section 4.2); (ii) an explore-exploit approach to probe the space of items, to allow continuous learning (Section 4.3) and (iii) a feedback elicitation mechanism for selecting absolute (Section 4.3) and relative questions (Section 4.4). </p>

<h4>4.1 Overview</h4>
<p>Our recommendation pipeline can be summarized as:</p>
<p>1. Pick a model (Absolute/Pairwise) (Sec. 4.2) and preference elicitation mechanism: Abs (Sec. 4.3) / Abs Pos / Abs Pos & Neg / Pairwise (Sec. 4.4).</p>
<p>2. Initialize model parameters using offine data.</p>
<p>3. A new user arrives. Now iterate for a few questions:</p>
(a) Mechanism selects a question to ask<br/>
(b) User answers the question<br/>
(c) All model parameters are updated<br/>
(d) Remove the question from the allowed questions<br/>
<p>4. System presents the final recommended list</p>
<p>The inner loop represents the ＆human in the loop＊ present in all interactive systems, i.e., we make an intervention which affects the user and thus the future system decisions.</p>

<h4>4.2 Latent Factor Recommendation</h4>
<p>Consider how people make recommendations when a friend asks them to suggest a restaurant. They strategically ask each question with the goal of eliminating or confirming strong candidates for dining out. Similarly, when designing a conversational recommender that asks questions about explicit items, a principled way of selecting items is desired.</p>
<p>The implicit structure of the item space that allows us to learn quickly is motivated by collaborative filtering. Items that have been co-rated similarly (liked/disliked) by users will lie close in a low dimensional embedding. For our model we use a simplified version of the Matchbox Recommender model [29] 每 equivalent to Probabilistic Matrix Factorization (PMF) [21]. This is a generative model, in that it specifies a probabilistic procedure by which the observed likes/dislikes of users on items are generated on the basis of latent variables. The model variables are learned so that the model can explain the observed training data.</p>
<p>Formally, throughout the paper we use the convention that i denotes the index over M users, forming the set I, and j denotes the index over N items, forming the set J . Every user i ﹋ I is modeled by a user bias variable alpha_i ﹋ R, accounting for users who tend to like/dislike most of the items, and a d-dimensional trait vector u_i ﹋ R^d. The trait vector represents the latent embedding of user i in a d-dimensional space, where d << M,N. Every item j ﹋ J is modeled with latent variables beta_j ﹋ R (the item bias that accounts for item popularity) and a trait vector v_j ﹋ R^d that represents the latent embedding of item j in the same d-dimensional space. </p>
<p>Given that both users and items trait vectors lie in the same latent space, the similarity between a user i and an item j can be measured with the inner product of their corresponding trait vectors u^T_i v_j . We now present two models for estimating the latent variables, depending on the type of observations we obtain from users, i.e., absolute or pairwise. </p>

<p>Pairwise Model. People are often better at giving relative preferences over items instead of absolute judgments. Such preferences yield tuples of the form (user i, item j, item h) when user i prefers item j over item h (both j and h are indices over J .) We can adapt the generative model to such ground truth data by modifying the third step of the Absolute model to yield a pairwise model: </p>


<h4>......</h4>

<h4>4.3 Continuous Learning Recommender</h4>
<p>To generate recommendations, most latent factor based recommender systems use an offline trained model based on past interactions, such as the one described above, that they periodically update to incorporate new data. The parameters for new users are typically initialized based on a combination of explicit attributes and past ratings (e.g [1]). The items with the highest predicted noisy affinity mean comprise the recommendations presented to the user.</p>
<p>In contrast, recent work has moved towards continuously learning recommenders [4, 16, 33]. This optimizes for online performance using explore-exploit (EE) strategies. We present our fully online updated recommendation approach with the embedded preference elicitation in Algorithm 1. Following we detail the components of this algorithm for the case of asking absolute questions (Abs). In Section 4.4 we show how we extend this framework for relative questions.</p>

<h5>4.3.1 Initialization from Offline Data</h5>

<p>We propose to initialize online learning models using an initial embedding, that is learned offine. We hypothesize that such an initial embedding will allow the system to learn new user＊s preferences more quickly. Effective learning from few questions is crucial for conversational recommenders. </p>

<p>We start by learning the offine embedding of items from logged observations. Then, we initialize the prior of every item j ﹋ J by setting the trait v_j and bias beta_j from the corresponding offine posterior 每 assuming that all items in the online phase appeared in the offine data. </p>

<p>For the initialization of the user parameters, we focus on the case when the user is new to the system. Without additional information, we can assume that the new user is similar to the average offine user. We implement this by using as trait and bias the mean value over all offine users: </p>
......

<h5>4.3.2 Question Selection Strategies</h5>
<p>When a new user initiates interaction with a continuous recommender, the system asks a few questions to learn about the user＊s preferences. During this phase, it is important to select questions that lead to learning effectively (i) the user＊s preferences and (ii) the questions＊ quality, so that the number of questions asked can be minimized and interactions remain enjoyable. This task can be modeled as an item selection task. Here, we propose approaches that capture characteristics of active learning and bandit learning. Active learning approaches capture the intuition that learning is fastest when the system queries for labels that provide a high amount of new information [26]. Bandit learning approaches balance the need to learn new information with a focus on what has already been learned [3, 5]. In the context of conversational recommenders, such a balance may help focus questions on the most relevant part of the latent space, while still considering that highly preferred items may lie in as of yet unexplored areas of the space.</p>
<p>The model＊s confidence in its belief over the user＊s preferences on items j ﹋ J at a given time is captured by the current variances of the posterior of the noisy anities y^cold_j. As we ask about an item j* and observe the user＊s feedback, the variance of the inferred noisy affinity of this item and of the nearby items in the learned embedding is reduced. Also, the means of these items＊ inferred noisy anities change. It is this property that allows us to search the space of items quickly. Hence, while in the classic multi-armed bandit scenario the bandit algorithms converge only after all arms are suciently explored, in our setting the collaborative structure allows for faster convergence.</p>

<p>We compare a number of approaches for question selection that reflect the considerations discussed above, and several baselines. All approaches are listed in Table 3. Each selects an item j* to obtain user feedback on. While the first few are self-explanatory baselines, we discuss three in more detail. (1) MaxT approximates maximizing information gain, in the vein of active learning: As user-item ratings are observed, the PMF model adds information to the corresponding user and item trait vectors. In the opposite extreme case, if all item trait elements are close to 0, the corresponding item carries no information. As MinT does the opposite from MaxT, it is hypothesized to have low performance and is employed to establish a lower bound on question selection performance. (2) UCB [3] is a popular bandit algorithm that selects the items with the highest confidence bound to avoid missing preferences for promising items. (3) Thompson Sampling (TS) is a bandit algorithm that balances exploration and exploitation by selecting items using a sampling strategy [5]. It samples from its current posterior belief over noisy anities, and then acts optimally according to this sampled belief. TS focuses on items with high mean affinity, but is also likely to select items with high variance. </p>

<h5>4.3.3 Online Updating</h5>
<p>After posing a question to the user, the observed response needs to be incorporated into to the recommender to allow for continued learning. As shown in [23], the questions can be incorporated into the model by setting the probability of the question to 1 and incorporating the user＊s response following standard probability theory. </p>
<p>The user＊s response thus becomes a new observation that the system uses to update the posterior distributions of all latent model parameters related to the incoming user i and the item j asked about, i.e., alpha_i, beta_j, u_i, v_j (but affecting only user i＊s interaction session). Due to space constraints, we refer the reader to [29] for the specific Expectation Propagation updates. To select the next question for user i, we use the updated posteriors as the new priors to infer the user＊s noisy affinity distribution ^y_{ij} for all items j ﹋ J , denoted by ^y_i. As the system keeps asking questions to user i and incorporates his/her feedback, the beliefs about the user, and the item in the question, change. This allows the model to move towards the true underlying affinity distribution. All online updates were implemented in Infer.NET [20]. </p>

<b>Abs: Absolute Model, Absolute Questions</b><br/>
<p>So far we have presented the entire framework for the case where the system poses absolute questions. Before turning to relative feedback, we describe the high-level approach for asking absolute questions (Abs). Using TS for illustration purposes, Abs asks user i about the item with the largest sampled noisy affinity as inferred by the Absolute model: </p>

Eqs 6<br/>

<p>Based on whether the user (dis)liked item j*, a new observation (i, j*,1/0) is incorporated into the Absolute model.</p>


<h4>4.4 Extension to Relative Preferences</h4>
<p>An alternative is for the system to ask for a preference about a pair of items, i.e., does the user prefer item A (j*) or item B (h*)? Therefore, we present here the extension of our framework to the case of asking relative questions.</p>
<p>We consider three separate formulations (referred to as Abs Pos, Abs Pos & Neg and Pairwise) for selecting relative questions and incorporating feedback, to identify the best way of asking relative questions. Importantly, in every such formulation there are two choices: (i) the underlying model (Absolute vs. Pairwise) and (ii) how the user＊s response to the question is incorporated back into the model. For the first choice, Abs Pos and Abs Pos & Neg use the Absolute model, while Pairwise uses the Pairwise model. The second choice is applicable only for Abs Pos and Abs Pos & Neg, as they represent two ways of incorporating relative feedback into the absolute model. </p>

<h5>4.4.1 Absolute Model, Relative Questions</h5>
<p>First, we present Abs Pos and Abs Pos & Neg. Both use the same mechanism to generate the question ※A vs. B§ for user i:</p>
1. Select item A as in Abs (Equation 6).<br/>
2. Virtual observation: Assume user i did not like A.<br/>
3. Virtual update: Incorporate the tuple (i, A, 0) into the Absolute model, infer the posteriors for all model parameters and set them as the virtual new prior.<br/>
4. Select item B, again according to Abs, but this time using the virtual prior as prior.<br/>

<p>The insight behind this mechanism of constructing the relative question is that the two items the user is asked to give a relative preference on should be relatively far apart in the latent embedding, so that (i) the system can learn users＊ preferences effectively and (ii) the user is not forced to choose among very similar items. This diversity enforcing mechanism is inspired by the approach in [6]. </p>
<p>The two methods introduced here differ only in the way the feedback is incorporated into the Absolute model. Abs Pos incorporates only positive information while Abs Pos & Neg incorporates both positive and negative information. For example, assume that the user preferred item B to A. Then, Abs Pos incorporates only the observation (i, B, 1), interpreting the relative preference on the preferred item B as a like for B. In contrast, Abs Pos & Neg incorporates two observations: (i, B, 1) for the preferred item and (i, A, 0) for the less preferred item. This can be seen as a variant of the ※sparring§ approach to dueling bandits [2], which samples item pairs and updates the model for both items as if an absolute reward were observed. </p>

<h5>4.4.2 Pairwise Model, Relative Questions</h5>
<p>The third method for selecting relative questions, Pairwise, uses the Pairwise model that directly takes pairwise preferences as input, to generate the relative question and incorporate the observations. Thus, the user＊s relative feedback is incorporated into the model without any intermediate transformation, i.e., as an observation (i, B, A) when B is preferred over A.</p>
<p>The Pairwise method picks A exactly as in Abs, and for item B, inspired by the dueling bandit approach in [34], it picks the item with the largest probability of being preferred to item A. We instantiate the latter by selecting the item with the maximum noisy difference from item A (j*): </p>
eqs 7<br/>
<p>For the selection of item B, any question selection strategy besides TS illustrated here (except for MinT, MaxT), can be employed exactly as in Table 3, with the difference that the noisy difference distribution should be used. </p>

<b>Incorporating the ＆Neither＊ Option.</b>
<p>Preliminary experiments showed that when the method asks the user to give a relative preference on two items that he dislikes, forcing him/her to choose one could mislead the model about the user＊s preferences. Thus, we adjusted all methods so that in such a case the user can specify that he likes neither. We implemented this by (i) incorporating two dislikes in Abs Pos & Neg and (ii) omitting the update in Abs Pos and Pairwise. </p>
<h3>5. EXPERIMENTAL SETUP</h3>
<p>We now describe our overall empirical setup, used for the experiments described in the next two sections.<br/>
Setting. One main diculty of evaluating conversational recommenders is that it requires the ability to have access to user reactions to any possible system action. We address this requirement using generative user models. The first user model is constructed synthetically and is used to validate our model (Section 6). The second is instantiated from real user preferences, collected via a user study (Section 7).</p>
<p>All experiments consist of an offine and an online phase. During the offine phase, the model is presented with data where M users interact with N items. In the subsequent online phase, the model is used to interact with cold-start users, asking questions using the pool of the offine N items.</p>
<p>We varied the number of questions from 0 to 15 and report the model＊s performance after each question. In practice, recommendations could be given after fewer questions, could be integrated with initial recommendations, or could be spread out over several interactions with a given user. </p>
<p><b>Research Questions.</b> Our experiments are designed to address the following research questions:
RQ 1. Can our model adapt to the user＊s preferences?<br/>
RQ 2. Does our model learn effectively under either absolute or relative feedback?<br/>
RQ 3. Which relative question method performs better?<br/>
RQ 4. Is absolute or relative feedback more effective?<br/>
RQ 5. Does the offine initialization step help?<br/>
RQ 6. Which question selection strategy is more effective?<br/>
To answer each one of these questions, we need some measure of evaluating the effectiveness of our framework. Given that the goal of preference elicitation is a good recommendation list adhering to the user＊s preferences, we use Average Precision@k (AP@k) as our evaluation metric.
</p>
<p>Metric. AP@k is a widely used, precision-oriented metric [15] for capturing accuracy in the top k (we set k = 10).</p>
<p>Formally, for user i, we obtain the user＊s predicted recommendation list by sorting all items by decreasing mean of inferred noisy anities y_i. We evaluate this list by looking at the ground truth r^true_i , i.e., capturing whether the user liked/disliked each item. AP@k is defined as the average of precisions computed at each liked position in the top k items of the user＊s ranked list. P@l (Precision@l) is the fraction of liked items out of the top l + 1 ranked items. Thus, </p>
eqs 8<br/>
<p>where [l] represents the index of the item present in rank l, with [l] = 0 corresponding to the index of the top recommended item. Higher value (closer to 1) of AP@k implies better recommendation list. In our results, we report the average and 95% confidence intervals of AP@10 over all cold-start users. Results in additional metrics, such as ratio of correctly ranked pairs and mean reciprocal rank, were omitted as they showed similar trends as AP@10. </p>
<h3>6. LEARNING SYNTHETIC USER TYPE PREFERENCES</h3>
<p>We begin our experiments with an intentionally simplistic example of restaurant recommendation, as real world high-dimensional data is dicult to visualize. The example is meant to demonstrate concepts of our model and to illustrate the effectiveness of our approach to unlearn initial prior beliefs and tailor recommendations to specific user types, answering RQ1 armatively.</p>
<p>In our framework, we first use observations to learn an offline embedding for users and items in the same low-d space. Here, we generated the offine observations by considering types of users and restaurants as follows:</p>
tables...<br/>
<p>We generated N = 200 restaurants, and M = 200 users. For each offine user, according to their type, we sampled 10 items from their liked category as likes and 10 items from the rest of the categories as dislikes. We used this intentionally simple synthetic setup to evaluate various parameter choices, and we show results for sigma_1^2 = 10, sigma_2^2 = 0.5, epslon = 0.1. To allow visualization, we considered only two latent traits (d=2) for this example. We see that in the learned embedding, the first trait indicates spiciness, while the second the price.</p>
<p>In the same space, an embedding for users is also learned (not shown to avoid clutter). The average trait vector over all users is shown with a red cross. This becomes the initial trait vector for the cold-start user. Considering also the learned items＊ biases and the average user bias (not shown here), the system constructs an initial estimate of the noisy affinity distribution of the incoming user about all items.</p>
<p>Based on the offine observations, the learned prior for this affinity distribution favors user types which were popular in the offine data. The task of selecting restaurants for online users resembling the mean offine user is easy, as the prior already captures valuable information. As Fig. 1, bottom right panel shows, even with no questions, a close to perfect recommendation list can be given for Liking not-spicy users. Similar is the trend for the Liking cheap users (not shown). </p>
<p>Figure 1: Results on synthetic restaurant data, across user types (left), and two of the user types (right). </p>
<p>In contrast, when the user is of a type that was rarely seen during the offine phase (e.g., expensive, shown in Fig. 1, top right panel), the online recommender has to collect observations that move away from more popular types. The trends for Liking spicy, Liking only-spicy, and only not-spicy user types are similar to the Liking expensive. For these types, the model (all four approaches) starts with AP@10 close to 0, but after every question asked, unlearns the initial wrong prior beliefs, and learns the specific user types＊ preferences. </p>
<p>For the results reported, we considered 60 cold-start users of each type and used TS for the question selection. </p>
<p>All methods learn effectively across all user types, with minor differences (Fig. 1, left) answering RQ 2 positively. </p>

<h3>7. RESULTS ON REAL DATA</h3>
<p>Having positively answered RQ 1 and RQ 2 above, we turn our attention to a real-world setting to address all research questions in the context of ＆where to dine in Cambridge, UK＊. For the offine initialization of our framework, we use real users＊ online search behavior in a commercial search engine (Section 7.1). We use the insights from Section 3 to design a user study that is used to collect real restaurant preferences for Cambridge (Section 7.2). The collected responses serve as a basis for evaluating our online recommendation methods. We sketch a novel two-step approach to infer ratings for all restaurants, apart from those asked in the study (Section 7.3). We extensively evaluate our choices for the recommendation pipeline (Section 7.4). </p>

<h4>7.1 Search Data for Offline Initialization</h4>
<p>We start by describing the data which serve as our offine user-restaurants observations, based on which we learn the embedding used to warm start the question-asking phase.</p>
<p>This data is obtained by identifying restaurant review pages for restaurants in Cambridge (UK) on a major restaurant review service provider. Next, we filter the query and click logs from a major commercial search engine to find (anonymous) cookies that mark a single PC accessing a sequence of these restaurant pages. Each cookie is taken to be a distinct user, and all visits on restaurant review pages are considered to be indicating the user liking the restaurant. </p>
<p>In particular, taking logs from 26 March to 26 April 2015, we identified 3,549 cookies (users) who accessed at least one of the 512 distinct Cambridge restaurant review pages identified on the review service provider. This resulted in an index of Cambridge restaurants, each one visited by at least one user. Augmenting each of the restaurants with all known links and metadata associated with it in a proprietary restaurant index, and selecting a further three months back in each of those users＊ search histories, we recorded every interaction of these users with these restaurant links. During the four month search history of the users, we recorded interactions with 289 unique restaurants out of the 512 Cambridge restaurants. The total number of unique user 每 restaurant interactions recorded is 9330.</p>
<p>Thus, our offine data consists of M = 3549 users, N = 289 restaurants, and 9330 positive observations (1). To introduce dislikes (0) to the rating matrix as well, for every user i who has liked n^+_i items, we sampled uniformly at random n^-_i = min(10, n^+_i ) restaurants as dislikes.</p>
<p>Parameter Setting. To learn the offine embedding, we set the hyper-parameters to the combination that achieved the highest pairwise accuracy in the offine observations: d = 4 (i.e., 4 latent traits), sigma_1^2 = sigma_2^2 = 10, epslon = 0.1.  </p>

<h4>7.2 User Study as Basis for Online Evaluation</h4>
<p>One of the issues of evaluating a conversational recommender is that one needs to know the user＊s ground truth on the space of all items (and questions). To obtain this, one needs to implement an interactive recommender asking questions to real users and receiving their responses. As an intermediate step, we conducted a user study and used the collected responses as ground truth for online users.</p>
<p>In the user study conducted, each participant filled in an anonymous web questionnaire about their preferences on a pool of restaurants in Cambridge, UK. The participants were asked ※would you consider restaurant X for your next Friday night dinner?§, labeling each restaurant with a binary label (yes/no). These responses comprise our ground truth.</p>
<p>For the pool of Cambridge restaurants we carefully selected ten restaurants, diverse in various features (as identified in Section 3). We recruited twenty eight individuals for the study. Given the anonymity of the questionnaire (in order to encourage truthfulness in the responses), demographic information was not recorded. However, the larger pool of in dividuals from which the participants were drawn (65 people working in a research lab) varied in factors such as age, job level, income, time spent in Cambridge.</p>
<p>Each participant was presented with the questionnaire about the same restaurants, but with varying order to avoid presentation bias. The participants were advised to visit the restaurant webpage when unfamiliar with the restaurant.</p>

<h4>7.3 Obtaining Ground Truth</h4>
<p>In our user study, we obtained restaurant labels for 10 out of the 289 Cambridge restaurants present in the offine data, for 28 participants. However, for reliable evaluation we need labels for the entire item space (rather than limiting our methods to ask about just these 10 restaurants). Therefore, we introduce an approach to fill in complete ground truth labels. Also, to increase the diversity of the user space, inspired by [17], we used bootstrapping to obtain 50 coldstart users based on the 28 participants＊ ground truth.</p>
<p>In particular, for each cold-start user: <br/>
1. Randomly sample one of the 28 participants.<br/>
2. Observe the sampled user＊s labels on the pool of 10 restaurants asked in the user study.<br/>
3. Infer user＊s traits u_i (prior= learned embedding in 7.1).<br/>
4. Sample ^u_i ~ u_i. Set this to be the new prior of u_i.<br/>
5. With this prior, infer the ratings r_i distribution.<br/>
6. Sample ratings from their distribution ^r_i ~r_i. In this way, for each bootstrapped user we obtain a complete rating list for all 289 restaurants that is consistent with the user study labels of some user, yet is perturbed to account for variety in real user populations.</p>
<p>As far as we are aware, this approach for filling in the missing ratings is novel. It gives us ground truth as close to real as possible given the resources available. Alternatives would be exhaustive labels (not feasible for our study), or rejection/importance sampling (only effective when leveraging logged exploration data from large-scale systems, e.g [16]).</p>

<h4>7.4 Results</h4>
<p>Having obtained the offine embedding and the online users＊ ground truth for all items, we now present our experiments on a real restaurant recommendation scenario with the focus of answering the remaining research questions RQ 3 - RQ 6. Each of these questions investigates a separate component of our continuously learning recommender system. </p>
<h4><i>Which method for relative questions is better? (RQ 3)</i></h4>
<p>Recall that we proposed three approaches for modeling relative feedback. The first two incorporate feedback in an absolute model, (a) by incorporating information that the user liked the preferred item (ignoring the non-preferred one, Abs Pos), and (b) by incorporating both positive (preferred) and negative (non-preferred) information (Abs Pos & Neg). Alternatively, we construct a pairwise model (Pairwise). The results of the three methods＊ comparison are shown in the left panel of Figure 2. These results were obtained using TS for question selection and start from the offine embedding. </p>
<p>We see no significant difference among the methods during the first few questions. After only 2 questions, all methods significantly improve over the initial performance of .584 to respectively .734 (Abs Pos), .780 (Abs Pos & Neg), and .684 (Pairwise). However, as we ask more questions, Abs Pos & Neg forces negative observations on liked restaurants, thus causing the method to degrade the ranking. Overall, Abs Pos is the most effective method for relative questions. </p>

<h4><i>Are absolute or relative questions better? (RQ 4)</i></h4>
<p>To answer this question, we compare the performance of the absolute-question asking method (Abs) with the best relative-question asking method (Abs Pos). The results are shown in the right panel of Figure 2. Until 2 questions, both methods have almost identical performance. But, after 5 questions, Abs performs significantly better than the relative feedback method, and achieves close to perfect performance after 15 questions (AP@10 = .975). We hypothesize that this result can be explained by the fact that our offine embedding favored absolute feedback.</p>
<p>Although our result shows that very high accuracy can be achieved when users provide accurate feedback on absolute questions, in practice this may not always be possible. Psychological effects such as anchor bias [12] can lead users to implicitly compare items, lowering the quality of absolute feedback. When this is the case, our result shows that high performance can be achieved with relative feedback as well. Future work could involve hybrid models that automatically learn whether absolute or relative feedback, or a combination, is more accurate in a given setting.</p>

<h4><i>Does offline initialization help? (RQ 5)</i></h4>
<p>Next, we investigate the impact of model initialization, i.e., initializing the online recommender with an initial offine embedding, compared to starting from a generic prior (using the optimized hyper-parameters specified in 7.1). We present the results of this comparison in Figure 3 both for the absolute (Abs) and the best relative feedback model (Abs Pos), in the left and right panel correspondingly. </p>
<p>Our hypothesis is that the offine embedding learned from the weakly labeled data of a search log captures sucient information to helpfully constrain continued learning, even if it does not exactly match the structure that underlies online users. Indeed, Figure 3 demonstrates great performance improvements when initializing the models from this embedding over generic prior initialization. By placing the new users as the average o?ine user, performance increases from .217 to .584, even without asking any questions. As the recommender collects more responses, performance continues to improve in both cases. </p>
<p>One observation is that the uninitialized system can ultimately achieve high performance, and for Abs Pos (Prior) even pass the o?ine initialized system. However, this is only achieved after many questions (here: 14). The phenomenon is nevertheless interesting, as it may point to a bias-variance trade-o?. Starting from the generic prior allows the system to eventually perfectly fit a given user＊s preferences, however, learning takes a long time because there is no initial structure to constrain learning outcomes. We conclude that using offine initialization is highly beneficial. </p>


<h4><i>Which question selection strategy is best? (RQ 6)</i></h4>
<p>In Figure 4 we report the comparison results of the various question selection strategies of Section 4.3.2 (except MaxV whose performance almost coincides with UCB), for the Abs and Abs Pos methods initialized with the offine embedding.</p>
<p>For the Abs model (Figure 4, top), we observe that: (i) as expected, lowest AP@10 is achieved by MinT, (ii) Random learns slowly, likely because it fails to focus on more promising items for effective learning, and (iii) all remaining strategies perform equally well. We hypothesize that Greedy performs well thanks to the offine embedding, along with the online updating of all parameters after each response.</p>
<p>Turning to Abs Pos (Figure 4, bottom), the best performing strategies are those that encourage more diversity across the questions of the interactive session, namely the banditbased ones and Random. Greedy and MaxT are the worst performing ones, following MinT. Our insight why this is the case is that they tend to select questions A vs B, followed by A vs C, etc. when A is preferred. Given that there is no construction encouraging B and C to be diverse (such as sampling or taking into account uncertainties), the questions focus on comparing parts of the embedding which are similar across questions, thus preventing truly effective learning.</p>
<p>Overall, we find that the bandit-inspired strategies perform the most robustly, achieving top performance across models. In the classic bandit setting, these approaches systematically balance the need to explore new solutions with the need to reap the rewards of what has already been learned. Here, we find that similar principles allow these strategies to collect user feedback that balances the need to not discard any areas of the latent space prematurely with the need to focus questions on the most promising areas. This novel insight is important because it shows that banditbased question selection strategies can lead to benefits that go beyond the typical bandit problem.</p>
<p><b>Discussion.</b> All our results show that our methods enable effective learning from interactions with their users, under either feedback type, answering positively RQ 1 and RQ 2.</p>
<p>We show substantial recommendation performance improvements over the performance we would get without adapting to the user; by 25% after only 2 questions.</p>
<p>Although our underlying model can be augmented with external features [29], one key advantage is it does not need them. It learns online both the user＊s and the items＊ latent traits. Together with [18] and [33], our findings corroborate the effectiveness of latent-feature interactive recommenders.</p>
<p>A novel insight is that taking into account the uncertainties in the learning of both the item and the user embedding, we can adapt to the specific user＊s preferences, under a certain context. This answers to the question posed by [33] and is key for allowing the system to learn both the user＊s profile and the questions＊ effectiveness in a contextual manner.</p>
<p>We have demonstrated effective learning from feedback present in many interactive settings. Thus, our approach is a good candidate for online learning across domains.</p>
<h3>8. CONCLUSIONS</h3>
<p>In this paper we proposed a novel view of recommendation as an interactive process. Like human recommenders, we envision recommender systems that can converse with new users to learn to know their preferences.</p>
<p>We develop such a conversational recommender, using restaurant recommendation as our motivating example. We start by examining restaurant related queries issued in a commercial search engine. Using these insights, we conducted a user study to elicit ground truth for evaluating our system. We propose a conversational recommender model that is theoretically well anchored in probabilistic matrix factorization models [21, 29]. We show how such models can be extended to support continuous learning. We empirically evaluate our approach using the ground truth based on real dining preferences elicited through our user study.</p>
<p>Our results have important implications for the development of conversational recommender systems. First, we demonstrated that best performance can be achieved with absolute questions. However, even in settings where only relative feedback is available, effective learning is possible. Second, we proposed a systematic approach to initializing conversational recommenders with an offine learned embedding, boosting greatly the performance even when only weakly supervised data is available. Third, we identified question selection strategies that can elicit feedback for very effective learning. Together, these insights pave the way towards conversational recommenders.</p>
<p>A promising future direction is to extend conversational recommenders to use reinforcement learning approaches, for capturing longer-term dependencies [17]. The modular structure of our framework allows various choices in a plug-andplay-manner, considering different feedback types, underlying probabilistic models etc., with the goal of building a suite of conversational recommenders for a variety of settings.</p>
<p><b>Acknowledgments.</b> We would like to thank Tom Minka and Yordan Zaykov for the very insightful discussions on the model and evaluation. We also thank the participants of our user study.</p>










